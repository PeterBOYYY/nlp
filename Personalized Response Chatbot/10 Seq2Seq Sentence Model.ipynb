{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preppy import Preppy\n",
    "from seq2seq import Model\n",
    "from tensorflow.contrib.seq2seq import *\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_size': 50,\n",
    "    'vocab_size': 0,\n",
    "    'hidden_size': 64,\n",
    "    'n_layers': 1,\n",
    "    \n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    'grad_clip': 5.0,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    'save_path' : './Model/Seq2seq/model.ckpt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataset by reading the train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x):\n",
    "    x['length'] = tf.expand_dims(tf.convert_to_tensor(x['length']),0)\n",
    "    return x\n",
    "\n",
    "def deflate(x):\n",
    "    x['length'] = tf.squeeze(x['length'])\n",
    "    return x\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def save_params(params):\n",
    "    with open('./Model/Seq2seq/params.pkl', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "def load_params():\n",
    "    with open('./Model/Seq2seq/params.pkl', 'rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preppy = pickle.load(open('./data/seq2seq/preppy.pkl','rb'))\n",
    "params[\"vocab_size\"] = len(preppy.vocab)\n",
    "tf.reset_default_graph()\n",
    "dataset_train = tf.data.TFRecordDataset(['./data/seq2seq/train.tfrecord']).map(preppy.parse)\n",
    "dataset_val = tf.data.TFRecordDataset(['./data/seq2seq/val.tfrecord']).map(preppy.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': TensorShape([Dimension(None)])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train = dataset_train.padded_batch(32,padded_shapes={\n",
    "    \"sentence\":tf.TensorShape([None])\n",
    "})\n",
    "\n",
    "batched_val = dataset_val.padded_batch(32,padded_shapes={\n",
    "    \"sentence\":tf.TensorShape([None])\n",
    "})\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, batched_train.output_types, batched_train.output_shapes)\n",
    "\n",
    "next_item = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': TensorShape([Dimension(None), Dimension(None)])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_train.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_train = batched_train.make_initializable_iterator()\n",
    "iterator_val = batched_val.make_initializable_iterator()\n",
    "\n",
    "handle_train = sess.run(iterator_train.string_handle())\n",
    "handle_val = sess.run(iterator_val.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Model(next_item, params)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/Seq2seq/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, params[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training\n"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "loss_val = []\n",
    "flag = 0\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    print(\"Epoch: %d\"%(epoch))\n",
    "    sess.run(iterator_train.initializer)\n",
    "    print(\"Training\")\n",
    "    while True:\n",
    "        try:\n",
    "            sen, logits, _, loss = sess.run([M.sentence, M.logits, M.train_op, M.loss],feed_dict={handle: handle_train, M.lr: params[\"learning_rate\"]})\n",
    "            loss_train.append(loss)\n",
    "            #print(loss)\n",
    "            #print(np.shape(sen))\n",
    "            #print(np.shape(logits.rnn_output))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        except tf.errors.DataLossError:\n",
    "            break\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    sess.run(iterator_val.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            loss, logits = sess.run([M.loss, M.logits], feed_dict={handle: handle_val})\n",
    "            loss_val.append(loss)\n",
    "            if flag == 0:\n",
    "                print(logits.sample_id)\n",
    "                flag = 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        except tf.errors.DataLossError:\n",
    "            break\n",
    "\n",
    "print(\"Training and Validation Finish\")\n",
    "\n",
    "# Save Model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, params[\"save_path\"])\n",
    "save_params(params[\"save_path\"])\n",
    "\n",
    "print('Model Trained and Saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.sample_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python36)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
