{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preppy import UserPreppy\n",
    "from user import UserModel\n",
    "from seq2seq import Seq2SeqModel\n",
    "from tensorflow.contrib.seq2seq import *\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_size': 50, # user embedding\n",
    "    'num_users': 101,\n",
    "    'hidden_size': 64, # dense layer\n",
    "    \n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    'grad_clip': 5.0,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    'save_path' : './Model/User/model.ckpt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataset by reading the train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x):\n",
    "    x['label'] = tf.expand_dims(tf.convert_to_tensor(x['label']),0)\n",
    "    x['user'] = tf.expand_dims(tf.convert_to_tensor(x['user']),0)\n",
    "    return x\n",
    "\n",
    "def deflate(x):\n",
    "    x['label'] = tf.squeeze(x['label'])\n",
    "    x['user'] = tf.squeeze(x['user'])\n",
    "    return x\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def save_params(params, path='./Model/User/params.pkl'):\n",
    "    with open(path, 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "def load_params(path='./Model/User/params.pkl'):\n",
    "    with open(path, 'rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preppy = pickle.load(open('./data/user/preppy.pkl','rb'))\n",
    "dataset_train = tf.data.TFRecordDataset(['./data/user/train.tfrecord']).map(preppy.parse)\n",
    "dataset_val = tf.data.TFRecordDataset(['./data/user/val.tfrecord']).map(preppy.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': TensorShape([Dimension(None)]),\n",
       " 'user': TensorShape([]),\n",
       " 'label': TensorShape([])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train = dataset_train.map(expand).padded_batch(32,padded_shapes={\n",
    "    \"sentence\":tf.TensorShape([None]),\n",
    "    \"label\":1,\n",
    "    \"user\":1\n",
    "}, drop_remainder=True).map(deflate)\n",
    "\n",
    "batched_val = dataset_val.map(expand).padded_batch(32,padded_shapes={\n",
    "    \"sentence\":tf.TensorShape([None]),\n",
    "    \"label\":1,\n",
    "    \"user\":1\n",
    "}, drop_remainder=True).map(deflate)\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, batched_train.output_types, batched_train.output_shapes)\n",
    "\n",
    "next_item = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': TensorShape([Dimension(32), Dimension(None)]),\n",
       " 'user': TensorShape([Dimension(32)]),\n",
       " 'label': TensorShape([Dimension(32)])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_train.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_train = batched_train.make_initializable_iterator()\n",
    "iterator_val = batched_val.make_initializable_iterator()\n",
    "\n",
    "handle_train = sess.run(iterator_train.string_handle())\n",
    "handle_val = sess.run(iterator_val.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/Seq2seq/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "seqParams = load_params('./Model/Seq2seq/params.pkl')\n",
    "seqParams[\"vocab_size\"] = len(preppy.vocab)\n",
    "\n",
    "Seq = Seq2SeqModel(next_item, seqParams)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, seqParams[\"save_path\"])\n",
    "\n",
    "params[\"sentence_size\"] = seqParams[\"hidden_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = UserModel(next_item, params)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64)\n"
     ]
    }
   ],
   "source": [
    "sentence = np.random.rand(params[\"batch_size\"], seqParams[\"hidden_size\"])\n",
    "sentence\n",
    "print(np.shape(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training\n",
      "0.675029\n",
      "0.6670722\n",
      "0.6523012\n",
      "0.72999626\n",
      "0.67252916\n",
      "0.63465667\n",
      "0.6983559\n",
      "0.62789303\n",
      "0.70183873\n",
      "0.7200966\n",
      "0.67391837\n",
      "0.6800141\n",
      "0.6531574\n",
      "0.7153834\n",
      "0.73229754\n",
      "0.7568157\n",
      "0.7057307\n",
      "0.75013375\n",
      "0.71488965\n",
      "0.7100783\n",
      "0.6504344\n",
      "0.6899806\n",
      "0.69403976\n",
      "0.69795763\n",
      "0.71849704\n",
      "0.6684686\n",
      "0.69949245\n",
      "0.70484734\n",
      "0.69305366\n",
      "0.6815133\n",
      "0.71252835\n",
      "0.68626654\n",
      "0.69254434\n",
      "0.65513647\n",
      "0.70442915\n",
      "0.6716073\n",
      "0.67620236\n",
      "0.7194283\n",
      "0.69569296\n",
      "0.68630713\n",
      "0.6994825\n",
      "0.69218695\n",
      "0.7085073\n",
      "0.67646694\n",
      "0.6676594\n",
      "0.66750515\n",
      "0.6959096\n",
      "0.70726824\n",
      "0.6777333\n",
      "0.66397953\n",
      "0.70961905\n",
      "0.6864778\n",
      "0.7026472\n",
      "0.6707838\n",
      "0.6573089\n",
      "0.70112824\n",
      "0.70692956\n",
      "0.7142452\n",
      "0.6942569\n",
      "0.69936097\n",
      "0.64874434\n",
      "0.70411026\n",
      "0.72077584\n",
      "0.652691\n",
      "0.7388009\n",
      "0.6924424\n",
      "0.7054165\n",
      "0.7161866\n",
      "0.67414474\n",
      "0.6438148\n",
      "0.67217845\n",
      "0.6782994\n",
      "0.6944267\n",
      "0.6708162\n",
      "0.6969852\n",
      "0.7248253\n",
      "0.7285496\n",
      "0.7161125\n",
      "0.6927978\n",
      "0.6917727\n",
      "0.70366836\n",
      "0.6835671\n",
      "0.6898908\n",
      "0.7140877\n",
      "0.7158962\n",
      "0.66463137\n",
      "0.7183664\n",
      "0.6874913\n",
      "0.7046777\n",
      "0.690469\n",
      "0.7161585\n",
      "0.69594705\n",
      "0.6834272\n",
      "0.69039017\n",
      "0.698817\n",
      "0.69753337\n",
      "0.6947134\n",
      "0.6955373\n",
      "0.67286897\n",
      "0.6945063\n",
      "0.6938766\n",
      "0.6968057\n",
      "0.67724425\n",
      "0.687695\n",
      "0.69023806\n",
      "0.6988922\n",
      "0.69887733\n",
      "0.68829584\n",
      "0.7033463\n",
      "0.6805645\n",
      "0.665727\n",
      "0.6896654\n",
      "0.69095755\n",
      "0.6832693\n",
      "0.69282424\n",
      "0.7060488\n",
      "0.67759496\n",
      "0.67602694\n",
      "0.66962254\n",
      "0.71062344\n",
      "0.6993377\n",
      "0.69695437\n",
      "0.7068031\n",
      "0.7179825\n",
      "0.6940205\n",
      "0.6822238\n",
      "0.7157399\n",
      "0.6858169\n",
      "Validation\n",
      "Epoch: 1\n",
      "Training\n",
      "0.6937396\n",
      "0.6700829\n",
      "0.6638262\n",
      "0.7046244\n",
      "0.67571545\n",
      "0.66314673\n",
      "0.7003478\n",
      "0.66008097\n",
      "0.69455934\n",
      "0.70221215\n",
      "0.6764984\n",
      "0.68215185\n",
      "0.66327226\n",
      "0.6994071\n",
      "0.7046711\n",
      "0.7221483\n",
      "0.6984128\n",
      "0.7241023\n",
      "0.70702934\n",
      "0.70487905\n",
      "0.6618451\n",
      "0.6811013\n",
      "0.68761337\n",
      "0.690457\n",
      "0.7014958\n",
      "0.67679775\n",
      "0.6851353\n",
      "0.68365085\n",
      "0.68729043\n",
      "0.6818856\n",
      "0.698534\n",
      "0.6888516\n",
      "0.6898681\n",
      "0.65352786\n",
      "0.690541\n",
      "0.66902506\n",
      "0.6788998\n",
      "0.7180857\n",
      "0.68780524\n",
      "0.6917807\n",
      "0.69504803\n",
      "0.69518703\n",
      "0.69552493\n",
      "0.67188656\n",
      "0.6745059\n",
      "0.6608718\n",
      "0.6997138\n",
      "0.71046555\n",
      "0.6806233\n",
      "0.656286\n",
      "0.70396185\n",
      "0.6878283\n",
      "0.70526034\n",
      "0.669572\n",
      "0.6570523\n",
      "0.69267166\n",
      "0.702788\n",
      "0.7122875\n",
      "0.6834669\n",
      "0.68791425\n",
      "0.6462146\n",
      "0.6978913\n",
      "0.7217003\n",
      "0.6554139\n",
      "0.73408294\n",
      "0.69375193\n",
      "0.70548755\n",
      "0.7149596\n",
      "0.6691241\n",
      "0.6399243\n",
      "0.6668978\n",
      "0.6764015\n",
      "0.6946136\n",
      "0.6685947\n",
      "0.69470966\n",
      "0.72162807\n",
      "0.72298384\n",
      "0.70580894\n",
      "0.69239485\n",
      "0.6884322\n",
      "0.70353854\n",
      "0.6799496\n",
      "0.6979198\n",
      "0.7124904\n",
      "0.71432436\n",
      "0.6648547\n",
      "0.71473444\n",
      "0.6850497\n",
      "0.70427084\n",
      "0.69391024\n",
      "0.71536696\n",
      "0.69272375\n",
      "0.6868668\n",
      "0.68937886\n",
      "0.6980988\n",
      "0.69091475\n",
      "0.69566053\n",
      "0.6927091\n",
      "0.66906416\n",
      "0.6922461\n",
      "0.69193107\n",
      "0.69491506\n",
      "0.6724311\n",
      "0.6820288\n",
      "0.68579364\n",
      "0.7028384\n",
      "0.6914004\n",
      "0.68775916\n",
      "0.70619285\n",
      "0.6745628\n",
      "0.66077065\n",
      "0.6872705\n",
      "0.69117415\n",
      "0.6807226\n",
      "0.68830395\n",
      "0.71215856\n",
      "0.6761775\n",
      "0.67667127\n",
      "0.6611378\n",
      "0.7112305\n",
      "0.696066\n",
      "0.69545794\n",
      "0.7059189\n",
      "0.7128939\n",
      "0.6959378\n",
      "0.67843413\n",
      "0.7145375\n",
      "0.6794181\n",
      "Validation\n",
      "Epoch: 2\n",
      "Training\n",
      "0.6982095\n",
      "0.6626908\n",
      "0.66122055\n",
      "0.70493585\n",
      "0.6685829\n",
      "0.66280866\n",
      "0.7056935\n",
      "0.6626488\n",
      "0.69448656\n",
      "0.7035795\n",
      "0.67506146\n",
      "0.6818657\n",
      "0.65707755\n",
      "0.6973609\n",
      "0.7004145\n",
      "0.72121567\n",
      "0.6994891\n",
      "0.72349995\n",
      "0.70728666\n",
      "0.70970064\n",
      "0.66098815\n",
      "0.6781858\n",
      "0.68569595\n",
      "0.6903348\n",
      "0.69815636\n",
      "0.677985\n",
      "0.6786399\n",
      "0.67757636\n",
      "0.6879009\n",
      "0.6805035\n",
      "0.69389236\n",
      "0.68987197\n",
      "0.6885531\n",
      "0.64963734\n",
      "0.6853696\n",
      "0.6662307\n",
      "0.67857414\n",
      "0.7226058\n",
      "0.6856524\n",
      "0.69667435\n",
      "0.6946052\n",
      "0.69983494\n",
      "0.68740785\n",
      "0.66927516\n",
      "0.6759816\n",
      "0.6551889\n",
      "0.706612\n",
      "0.7145625\n",
      "0.68228066\n",
      "0.6479076\n",
      "0.7032131\n",
      "0.6902367\n",
      "0.70866287\n",
      "0.67110413\n",
      "0.6585364\n",
      "0.68951046\n",
      "0.70101637\n",
      "0.71254635\n",
      "0.6766368\n",
      "0.6787957\n",
      "0.6433778\n",
      "0.6945319\n",
      "0.72196376\n",
      "0.65836936\n",
      "0.7327546\n",
      "0.69612616\n",
      "0.7085048\n",
      "0.7131001\n",
      "0.66491675\n",
      "0.6363137\n",
      "0.66439384\n",
      "0.6765468\n",
      "0.6949343\n",
      "0.6670104\n",
      "0.6938814\n",
      "0.719176\n",
      "0.7181947\n",
      "0.6973839\n",
      "0.6931459\n",
      "0.6848481\n",
      "0.70458937\n",
      "0.67764574\n",
      "0.70339406\n",
      "0.7107095\n",
      "0.71027696\n",
      "0.66672194\n",
      "0.711293\n",
      "0.6835011\n",
      "0.70390093\n",
      "0.6967629\n",
      "0.71401197\n",
      "0.69120765\n",
      "0.6910527\n",
      "0.6874269\n",
      "0.6980489\n",
      "0.68653303\n",
      "0.6955054\n",
      "0.6906085\n",
      "0.6658828\n",
      "0.6898271\n",
      "0.69128466\n",
      "0.6916278\n",
      "0.67049575\n",
      "0.67855424\n",
      "0.6834817\n",
      "0.7081392\n",
      "0.6851697\n",
      "0.6871296\n",
      "0.7070036\n",
      "0.6705837\n",
      "0.6588336\n",
      "0.68583465\n",
      "0.691496\n",
      "0.67927486\n",
      "0.685634\n",
      "0.7176002\n",
      "0.67464864\n",
      "0.67814606\n",
      "0.6545132\n",
      "0.7107432\n",
      "0.69297457\n",
      "0.693979\n",
      "0.70579153\n",
      "0.7094538\n",
      "0.69936574\n",
      "0.67527306\n",
      "0.71311677\n",
      "0.67477757\n",
      "Validation\n",
      "Epoch: 3\n",
      "Training\n",
      "0.7021682\n",
      "0.658169\n",
      "0.65843385\n",
      "0.70519125\n",
      "0.6621038\n",
      "0.6631874\n",
      "0.70905066\n",
      "0.66431856\n",
      "0.69509083\n",
      "0.70547557\n",
      "0.674492\n",
      "0.68102133\n",
      "0.65074456\n",
      "0.6959841\n",
      "0.69818795\n",
      "0.72143763\n",
      "0.70040107\n",
      "0.72281396\n",
      "0.707242\n",
      "0.71487826\n",
      "0.6600455\n",
      "0.6761116\n",
      "0.6839734\n",
      "0.6890263\n",
      "0.6951303\n",
      "0.678163\n",
      "0.6726212\n",
      "0.6743792\n",
      "0.6877654\n",
      "0.67915225\n",
      "0.6909481\n",
      "0.6902765\n",
      "0.6882183\n",
      "0.6465852\n",
      "0.680941\n",
      "0.663276\n",
      "0.6784144\n",
      "0.72599036\n",
      "0.6838088\n",
      "0.70068276\n",
      "0.69378114\n",
      "0.70256734\n",
      "0.6817912\n",
      "0.6670171\n",
      "0.67532134\n",
      "0.6503659\n",
      "0.7127079\n",
      "0.7168226\n",
      "0.6832528\n",
      "0.64202154\n",
      "0.7035942\n",
      "0.6920765\n",
      "0.7111475\n",
      "0.67320585\n",
      "0.66040456\n",
      "0.68726385\n",
      "0.70031106\n",
      "0.7128795\n",
      "0.6713716\n",
      "0.67221296\n",
      "0.6423244\n",
      "0.6926528\n",
      "0.7220504\n",
      "0.6615958\n",
      "0.7321398\n",
      "0.6986149\n",
      "0.7121068\n",
      "0.711349\n",
      "0.6618356\n",
      "0.63403827\n",
      "0.6632386\n",
      "0.6774094\n",
      "0.6956074\n",
      "0.6661986\n",
      "0.6932417\n",
      "0.7170574\n",
      "0.71445477\n",
      "0.69103074\n",
      "0.69426227\n",
      "0.6822375\n",
      "0.705242\n",
      "0.6760307\n",
      "0.70745873\n",
      "0.70841163\n",
      "0.70647407\n",
      "0.6687486\n",
      "0.7085724\n",
      "0.68261355\n",
      "0.70358104\n",
      "0.69909585\n",
      "0.7125435\n",
      "0.68997145\n",
      "0.6947256\n",
      "0.6856923\n",
      "0.6978632\n",
      "0.6833061\n",
      "0.69524693\n",
      "0.68866616\n",
      "0.6638032\n",
      "0.68773854\n",
      "0.6908653\n",
      "0.6885217\n",
      "0.6699568\n",
      "0.6760987\n",
      "0.6820672\n",
      "0.71248376\n",
      "0.68038\n",
      "0.68665254\n",
      "0.7068081\n",
      "0.66812664\n",
      "0.6583375\n",
      "0.684826\n",
      "0.6914266\n",
      "0.6782707\n",
      "0.683799\n",
      "0.72135353\n",
      "0.67358893\n",
      "0.6796856\n",
      "0.65000504\n",
      "0.7096504\n",
      "0.69050217\n",
      "0.6925242\n",
      "0.70594347\n",
      "0.7071269\n",
      "0.70250463\n",
      "0.67313343\n",
      "0.71180433\n",
      "0.67162365\n",
      "Validation\n",
      "Epoch: 4\n",
      "Training\n",
      "0.7052242\n",
      "0.65579504\n",
      "0.65627474\n",
      "0.70539856\n",
      "0.65726936\n",
      "0.66397405\n",
      "0.71119845\n",
      "0.6654736\n",
      "0.6958324\n",
      "0.7074287\n",
      "0.67457706\n",
      "0.6802065\n",
      "0.6456636\n",
      "0.6951872\n",
      "0.696939\n",
      "0.72227633\n",
      "0.70074606\n",
      "0.7223377\n",
      "0.70739126\n",
      "0.71902764\n",
      "0.659521\n",
      "0.67483026\n",
      "0.6825767\n",
      "0.6873087\n",
      "0.6928927\n",
      "0.6779194\n",
      "0.667749\n",
      "0.6731782\n",
      "0.68711007\n",
      "0.67870116\n",
      "0.6891662\n",
      "0.6900486\n",
      "0.68802834\n",
      "0.64444554\n",
      "0.67742944\n",
      "0.6610317\n",
      "0.6783099\n",
      "0.7280139\n",
      "0.6822519\n",
      "0.7037573\n",
      "0.6927556\n",
      "0.70398104\n",
      "0.67818606\n",
      "0.6651938\n",
      "0.6739868\n",
      "0.6464436\n",
      "0.7170028\n",
      "0.71797866\n",
      "0.6834916\n",
      "0.63833934\n",
      "0.70411825\n",
      "0.6931097\n",
      "0.71258724\n",
      "0.6748827\n",
      "0.6616646\n",
      "0.685369\n",
      "0.69993806\n",
      "0.71286714\n",
      "0.667516\n",
      "0.6675049\n",
      "0.6420797\n",
      "0.6915297\n",
      "0.72198725\n",
      "0.6643455\n",
      "0.73171425\n",
      "0.70049447\n",
      "0.7152326\n",
      "0.7097697\n",
      "0.6596792\n",
      "0.6326458\n",
      "0.66270953\n",
      "0.6783108\n",
      "0.69632745\n",
      "0.66580975\n",
      "0.6925744\n",
      "0.7153053\n",
      "0.71176887\n",
      "0.68657196\n",
      "0.6953572\n",
      "0.6805022\n",
      "0.7055241\n",
      "0.67489755\n",
      "0.7103206\n",
      "0.70614755\n",
      "0.70336807\n",
      "0.6704408\n",
      "0.70667195\n",
      "0.6820947\n",
      "0.7033173\n",
      "0.7009065\n",
      "0.71128166\n",
      "0.688967\n",
      "0.6976743\n",
      "0.6844584\n",
      "0.69754004\n",
      "0.68098915\n",
      "0.6951649\n",
      "0.6869644\n",
      "0.6626075\n",
      "0.68601775\n",
      "0.6905432\n",
      "0.68618375\n",
      "0.67002153\n",
      "0.67433864\n",
      "0.6810924\n",
      "0.7155696\n",
      "0.67689574\n",
      "0.6862744\n",
      "0.70623577\n",
      "0.66658497\n",
      "0.65841293\n",
      "0.6839342\n",
      "0.691084\n",
      "0.6774002\n",
      "0.6824802\n",
      "0.7237188\n",
      "0.672935\n",
      "0.68096447\n",
      "0.6470542\n",
      "0.7082933\n",
      "0.68863815\n",
      "0.69126606\n",
      "0.70617425\n",
      "0.7055078\n",
      "0.7049426\n",
      "0.67165476\n",
      "0.7106154\n",
      "0.6694194\n",
      "Validation\n",
      "Epoch: 5\n",
      "Training\n",
      "0.70745057\n",
      "0.6546657\n",
      "0.65459764\n",
      "0.70552146\n",
      "0.6537924\n",
      "0.66479766\n",
      "0.71248984\n",
      "0.6661482\n",
      "0.69653594\n",
      "0.709024\n",
      "0.67493623\n",
      "0.6795492\n",
      "0.6419014\n",
      "0.6947112\n",
      "0.69626296\n",
      "0.7234248\n",
      "0.70074147\n",
      "0.72209203\n",
      "0.7077204\n",
      "0.72202194\n",
      "0.6593004\n",
      "0.6741018\n",
      "0.6815229\n",
      "0.6856879\n",
      "0.6914898\n",
      "0.6775507\n",
      "0.6642041\n",
      "0.67325556\n",
      "0.68620825\n",
      "0.6789867\n",
      "0.6882572\n",
      "0.6895765\n",
      "0.6877786\n",
      "0.64307564\n",
      "0.67501974\n",
      "0.65958077\n",
      "0.6782242\n",
      "0.7292587\n",
      "0.68108106\n",
      "0.7060191\n",
      "0.6918817\n",
      "0.7046592\n",
      "0.6760123\n",
      "0.66382974\n",
      "0.67250425\n",
      "0.6432694\n",
      "0.7198328\n",
      "0.7186568\n",
      "0.68329114\n",
      "0.6361751\n",
      "0.7045764\n",
      "0.6933702\n",
      "0.7132853\n",
      "0.67592853\n",
      "0.66220635\n",
      "0.68364143\n",
      "0.69954216\n",
      "0.712546\n",
      "0.6647173\n",
      "0.66407967\n",
      "0.64205635\n",
      "0.69077885\n",
      "0.72183514\n",
      "0.66645294\n",
      "0.73135567\n",
      "0.7017019\n",
      "0.7176343\n",
      "0.70830095\n",
      "0.65814096\n",
      "0.63174284\n",
      "0.6623884\n",
      "0.6789549\n",
      "0.6969376\n",
      "0.6655579\n",
      "0.691855\n",
      "0.71390855\n",
      "0.7099199\n",
      "0.6835499\n",
      "0.696311\n",
      "0.67937565\n",
      "0.7055454\n",
      "0.67402637\n",
      "0.71229863\n",
      "0.70413166\n",
      "0.700934\n",
      "0.6717174\n",
      "0.705389\n",
      "0.6817937\n",
      "0.7030478\n",
      "0.7022482\n",
      "0.71030295\n",
      "0.6881825\n",
      "0.700089\n",
      "0.6836729\n",
      "0.6972179\n",
      "0.6793812\n",
      "0.6952673\n",
      "0.6855222\n",
      "0.6620019\n",
      "0.6846327\n",
      "0.69036037\n",
      "0.68465996\n",
      "0.6703295\n",
      "0.673102\n",
      "0.6803373\n",
      "0.7176592\n",
      "0.6743527\n",
      "0.685954\n",
      "0.70556253\n",
      "0.66556823\n",
      "0.65864223\n",
      "0.68309045\n",
      "0.69067156\n",
      "0.676622\n",
      "0.6814474\n",
      "0.72524774\n",
      "0.67255336\n",
      "0.6819652\n",
      "0.64516884\n",
      "0.70688105\n",
      "0.6872282\n",
      "0.6902671\n",
      "0.70644486\n",
      "0.70426184\n",
      "0.7067558\n",
      "0.67051625\n",
      "0.70949876\n",
      "0.6677439\n",
      "Validation\n",
      "Epoch: 6\n",
      "Training\n",
      "0.70910746\n",
      "0.6541505\n",
      "0.65319693\n",
      "0.70548236\n",
      "0.6512382\n",
      "0.6654335\n",
      "0.7132032\n",
      "0.6664574\n",
      "0.69718206\n",
      "0.7100894\n",
      "0.67536944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67900646\n",
      "0.6391311\n",
      "0.6943078\n",
      "0.6958252\n",
      "0.72457695\n",
      "0.70060146\n",
      "0.72195166\n",
      "0.7080943\n",
      "0.72410583\n",
      "0.6591858\n",
      "0.67370474\n",
      "0.6807076\n",
      "0.6843397\n",
      "0.69066775\n",
      "0.6771771\n",
      "0.6617391\n",
      "0.67397916\n",
      "0.6852362\n",
      "0.6796367\n",
      "0.6879277\n",
      "0.6891203\n",
      "0.6874554\n",
      "0.6423245\n",
      "0.67358875\n",
      "0.6587086\n",
      "0.6781608\n",
      "0.7302122\n",
      "0.6803287\n",
      "0.7076108\n",
      "0.69131994\n",
      "0.70499444\n",
      "0.6748015\n",
      "0.66290385\n",
      "0.67104226\n",
      "0.64070475\n",
      "0.7217897\n",
      "0.71922517\n",
      "0.682933\n",
      "0.63503206\n",
      "0.7050233\n",
      "0.69302446\n",
      "0.7135754\n",
      "0.67649794\n",
      "0.6623136\n",
      "0.68206656\n",
      "0.6990291\n",
      "0.7120718\n",
      "0.66265273\n",
      "0.66154486\n",
      "0.6420203\n",
      "0.6902468\n",
      "0.72163075\n",
      "0.6680323\n",
      "0.7310366\n",
      "0.7023934\n",
      "0.71936786\n",
      "0.7068995\n",
      "0.65701514\n",
      "0.6311375\n",
      "0.6620907\n",
      "0.67925394\n",
      "0.6974059\n",
      "0.6652703\n",
      "0.6911301\n",
      "0.71285224\n",
      "0.70865273\n",
      "0.6815282\n",
      "0.697068\n",
      "0.67858315\n",
      "0.7053782\n",
      "0.67324555\n",
      "0.7136486\n",
      "0.7023705\n",
      "0.6990537\n",
      "0.67261755\n",
      "0.7044811\n",
      "0.68162394\n",
      "0.70274764\n",
      "0.7031907\n",
      "0.7095821\n",
      "0.68757445\n",
      "0.7021304\n",
      "0.68317604\n",
      "0.69697726\n",
      "0.6782762\n",
      "0.6954262\n",
      "0.68432134\n",
      "0.66174316\n",
      "0.6835599\n",
      "0.69032013\n",
      "0.68377787\n",
      "0.670705\n",
      "0.6722779\n",
      "0.6797426\n",
      "0.7190971\n",
      "0.6724335\n",
      "0.6856531\n",
      "0.70490026\n",
      "0.6648896\n",
      "0.6588366\n",
      "0.6823633\n",
      "0.6903381\n",
      "0.67599446\n",
      "0.68059564\n",
      "0.7263417\n",
      "0.6723605\n",
      "0.6827596\n",
      "0.644011\n",
      "0.70555365\n",
      "0.6861611\n",
      "0.6895484\n",
      "0.70676976\n",
      "0.7032428\n",
      "0.7081237\n",
      "0.66958296\n",
      "0.70845\n",
      "0.66640115\n",
      "Validation\n",
      "Epoch: 7\n",
      "Training\n",
      "0.7104256\n",
      "0.6539245\n",
      "0.65198994\n",
      "0.7052659\n",
      "0.6493418\n",
      "0.66582143\n",
      "0.7135548\n",
      "0.6665738\n",
      "0.6977836\n",
      "0.7106628\n",
      "0.67582566\n",
      "0.6785046\n",
      "0.6370467\n",
      "0.69388795\n",
      "0.6953887\n",
      "0.72550297\n",
      "0.7004341\n",
      "0.7218056\n",
      "0.7083963\n",
      "0.7255676\n",
      "0.6590277\n",
      "0.67347425\n",
      "0.67999744\n",
      "0.68327713\n",
      "0.6901324\n",
      "0.6768312\n",
      "0.65997267\n",
      "0.67486215\n",
      "0.6843076\n",
      "0.68031573\n",
      "0.6878864\n",
      "0.6887485\n",
      "0.687093\n",
      "0.6420194\n",
      "0.67281854\n",
      "0.65814114\n",
      "0.67810804\n",
      "0.73107886\n",
      "0.6799326\n",
      "0.70866156\n",
      "0.6910531\n",
      "0.7051822\n",
      "0.67422223\n",
      "0.6623175\n",
      "0.6696483\n",
      "0.63865477\n",
      "0.7232889\n",
      "0.7198087\n",
      "0.68256354\n",
      "0.63460207\n",
      "0.70554173\n",
      "0.6922971\n",
      "0.71371114\n",
      "0.6768208\n",
      "0.662292\n",
      "0.6807065\n",
      "0.6984695\n",
      "0.7116082\n",
      "0.6611071\n",
      "0.6596842\n",
      "0.64191926\n",
      "0.6899031\n",
      "0.7214126\n",
      "0.66923964\n",
      "0.7307767\n",
      "0.7027639\n",
      "0.72059625\n",
      "0.7055925\n",
      "0.6562021\n",
      "0.6307565\n",
      "0.6617725\n",
      "0.6792387\n",
      "0.6977697\n",
      "0.66487455\n",
      "0.6904723\n",
      "0.712145\n",
      "0.7077642\n",
      "0.6801709\n",
      "0.6976082\n",
      "0.67791045\n",
      "0.7050843\n",
      "0.6724807\n",
      "0.71455324\n",
      "0.7008143\n",
      "0.6976168\n",
      "0.6732176\n",
      "0.70376295\n",
      "0.681527\n",
      "0.70245886\n",
      "0.7038133\n",
      "0.7090455\n",
      "0.6870775\n",
      "0.7038446\n",
      "0.68281245\n",
      "0.69678545\n",
      "0.6774701\n",
      "0.6955081\n",
      "0.6833298\n",
      "0.66164327\n",
      "0.6827459\n",
      "0.69035137\n",
      "0.6832844\n",
      "0.6710567\n",
      "0.6717559\n",
      "0.6792992\n",
      "0.72012496\n",
      "0.6709354\n",
      "0.6853377\n",
      "0.70427084\n",
      "0.6644598\n",
      "0.658926\n",
      "0.68182385\n",
      "0.6901336\n",
      "0.6755489\n",
      "0.67990583\n",
      "0.7272016\n",
      "0.6722884\n",
      "0.683421\n",
      "0.6433347\n",
      "0.7043677\n",
      "0.6853545\n",
      "0.68910956\n",
      "0.70715904\n",
      "0.7024264\n",
      "0.70919204\n",
      "0.6688288\n",
      "0.7074897\n",
      "0.66533005\n",
      "Validation\n",
      "Epoch: 8\n",
      "Training\n",
      "0.7115643\n",
      "0.6538687\n",
      "0.6509831\n",
      "0.7049171\n",
      "0.64796907\n",
      "0.6660362\n",
      "0.71372867\n",
      "0.6666622\n",
      "0.6983663\n",
      "0.71088684\n",
      "0.6763202\n",
      "0.6780016\n",
      "0.63547254\n",
      "0.6934911\n",
      "0.69484365\n",
      "0.7260852\n",
      "0.70027745\n",
      "0.72161174\n",
      "0.70857334\n",
      "0.72663885\n",
      "0.65876305\n",
      "0.6733052\n",
      "0.6792897\n",
      "0.68246126\n",
      "0.68969107\n",
      "0.6765311\n",
      "0.65855676\n",
      "0.67558247\n",
      "0.68348676\n",
      "0.68079746\n",
      "0.6878935\n",
      "0.6884264\n",
      "0.68672144\n",
      "0.6419716\n",
      "0.6723646\n",
      "0.6576582\n",
      "0.6780389\n",
      "0.7318649\n",
      "0.67976165\n",
      "0.7093121\n",
      "0.69098055\n",
      "0.7052905\n",
      "0.674017\n",
      "0.661914\n",
      "0.66836035\n",
      "0.63702834\n",
      "0.72450936\n",
      "0.7203754\n",
      "0.68221426\n",
      "0.6346526\n",
      "0.70614845\n",
      "0.6914189\n",
      "0.71385705\n",
      "0.67706186\n",
      "0.6623007\n",
      "0.6796182\n",
      "0.6979963\n",
      "0.7112725\n",
      "0.6599449\n",
      "0.6583617\n",
      "0.6417695\n",
      "0.68974847\n",
      "0.72123766\n",
      "0.670179\n",
      "0.7306195\n",
      "0.70297134\n",
      "0.72150874\n",
      "0.70445144\n",
      "0.6556578\n",
      "0.6305603\n",
      "0.66144365\n",
      "0.6790006\n",
      "0.6980894\n",
      "0.66437477\n",
      "0.6899419\n",
      "0.7117939\n",
      "0.70712066\n",
      "0.6792536\n",
      "0.6979515\n",
      "0.6772438\n",
      "0.70473385\n",
      "0.67175096\n",
      "0.71516323\n",
      "0.69943285\n",
      "0.69654524\n",
      "0.67360246\n",
      "0.70314133\n",
      "0.6814687\n",
      "0.702258\n",
      "0.70420957\n",
      "0.7086166\n",
      "0.68662715\n",
      "0.7052258\n",
      "0.68247867\n",
      "0.6965443\n",
      "0.6767958\n",
      "0.6954509\n",
      "0.68252367\n",
      "0.66158044\n",
      "0.6821233\n",
      "0.69035655\n",
      "0.6829283\n",
      "0.6713477\n",
      "0.67141306\n",
      "0.67896605\n",
      "0.7208528\n",
      "0.66975665\n",
      "0.68500066\n",
      "0.7036449\n",
      "0.6642212\n",
      "0.65892625\n",
      "0.681476\n",
      "0.69002324\n",
      "0.67524505\n",
      "0.67939115\n",
      "0.72789073\n",
      "0.6722579\n",
      "0.6839886\n",
      "0.6429459\n",
      "0.7033043\n",
      "0.6847314\n",
      "0.6889151\n",
      "0.7075897\n",
      "0.7018164\n",
      "0.7100456\n",
      "0.6682448\n",
      "0.7066308\n",
      "0.6645\n",
      "Validation\n",
      "Epoch: 9\n",
      "Training\n",
      "0.71260864\n",
      "0.65395916\n",
      "0.65019953\n",
      "0.70449734\n",
      "0.6470267\n",
      "0.66621053\n",
      "0.7138716\n",
      "0.66683066\n",
      "0.69896\n",
      "0.7109114\n",
      "0.6768716\n",
      "0.6774993\n",
      "0.6343484\n",
      "0.6931944\n",
      "0.69419235\n",
      "0.726312\n",
      "0.70014215\n",
      "0.72138673\n",
      "0.70863324\n",
      "0.7274692\n",
      "0.6584139\n",
      "0.6731467\n",
      "0.6785426\n",
      "0.6818542\n",
      "0.6892915\n",
      "0.67631316\n",
      "0.657261\n",
      "0.6759805\n",
      "0.68279356\n",
      "0.68098223\n",
      "0.6877988\n",
      "0.68811214\n",
      "0.6863508\n",
      "0.64201456\n",
      "0.67198503\n",
      "0.6571425\n",
      "0.67793655\n",
      "0.732505\n",
      "0.67966455\n",
      "0.7097204\n",
      "0.69099486\n",
      "0.7053316\n",
      "0.6739578\n",
      "0.6615373\n",
      "0.66725165\n",
      "0.6357206\n",
      "0.7254678\n",
      "0.7208327\n",
      "0.6818671\n",
      "0.63495636\n",
      "0.70677674\n",
      "0.6905807\n",
      "0.7140949\n",
      "0.6772762\n",
      "0.66233\n",
      "0.67880195\n",
      "0.69771504\n",
      "0.7111015\n",
      "0.65907085\n",
      "0.65744704\n",
      "0.64159966\n",
      "0.68975806\n",
      "0.7211718\n",
      "0.6708871\n",
      "0.73059547\n",
      "0.70310545\n",
      "0.7222661\n",
      "0.7035394\n",
      "0.6553451\n",
      "0.6305001\n",
      "0.6611167\n",
      "0.67864424\n",
      "0.6984122\n",
      "0.6638175\n",
      "0.68955636\n",
      "0.711766\n",
      "0.7066448\n",
      "0.67865\n",
      "0.69814533\n",
      "0.67657125\n",
      "0.7043931\n",
      "0.6711208\n",
      "0.7156142\n",
      "0.69823146\n",
      "0.6957937\n",
      "0.6738422\n",
      "0.702608\n",
      "0.68144023\n",
      "0.7022021\n",
      "0.7044779\n",
      "0.70825213\n",
      "0.6861843\n",
      "0.7062895\n",
      "0.68213165\n",
      "0.69617045\n",
      "0.67615616\n",
      "0.69527566\n",
      "0.6819016\n",
      "0.6615066\n",
      "0.6816519\n",
      "0.6902701\n",
      "0.6825252\n",
      "0.671579\n",
      "0.67113847\n",
      "0.67865324\n",
      "0.7213038\n",
      "0.66885734\n",
      "0.6846759\n",
      "0.70297927\n",
      "0.6641253\n",
      "0.65891325\n",
      "0.6812519\n",
      "0.68993187\n",
      "0.6749915\n",
      "0.67905736\n",
      "0.7284169\n",
      "0.6721872\n",
      "0.68445957\n",
      "0.6426944\n",
      "0.70231354\n",
      "0.6842269\n",
      "0.6888876\n",
      "0.70800143\n",
      "0.7013873\n",
      "0.7107198\n",
      "0.6677921\n",
      "0.7058681\n",
      "0.6638535\n",
      "Validation\n",
      "Training and Validation Finish\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "loss_val = []\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    print(\"Epoch: %d\"%(epoch))\n",
    "    sess.run(iterator_train.initializer)\n",
    "    print(\"Training\")\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = sess.run([Seq.encoder_state],feed_dict={handle: handle_train})\n",
    "            sentence = sentence[0][1]\n",
    "            _, loss = sess.run([M.train_op, M.loss],feed_dict={handle: handle_train, M.lr: params[\"learning_rate\"], M.sentence: sentence})\n",
    "            loss_train.append(loss)\n",
    "            print(loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        except tf.errors.DataLossError:\n",
    "            break\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    sess.run(iterator_val.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            loss = sess.run([M.loss], feed_dict={handle: handle_val, M.sentence: sentence})\n",
    "            loss_val.append(loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        except tf.errors.DataLossError:\n",
    "            break\n",
    "\n",
    "print(\"Training and Validation Finish\")\n",
    "\n",
    "# Save Model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, params[\"save_path\"])\n",
    "save_params(params[\"save_path\"])\n",
    "\n",
    "print('Model Trained and Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python36)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
